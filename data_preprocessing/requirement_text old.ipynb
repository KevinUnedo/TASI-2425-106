{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\M.S.I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\M.S.I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rake_nltk import Rake\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Metric\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Modify filename if necessary)\n",
    "df = pd.read_csv(\"../Dataset/text_requirement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The system can file, store and retrieve inform...</td>\n",
       "      <td>BWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The system must allow a student to be tracked ...</td>\n",
       "      <td>BWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The system must support management of access, ...</td>\n",
       "      <td>BWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The system must associate CBT, WBT, and e-Lear...</td>\n",
       "      <td>BWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The system must associate test and examination...</td>\n",
       "      <td>BWR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_Text Label\n",
       "0  The system can file, store and retrieve inform...   BWR\n",
       "1  The system must allow a student to be tracked ...   BWR\n",
       "2  The system must support management of access, ...   BWR\n",
       "3  The system must associate CBT, WBT, and e-Lear...   BWR\n",
       "4  The system must associate test and examination...   BWR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure correct column names\n",
    "df.columns = ['Review_Text', 'Label']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The system file, store retrieve information ac...\n",
       "1    The system must allow student tracked UEID pro...\n",
       "2    The system must support management access, vie...\n",
       "3    The system must associate CBT, WBT, e-Learning...\n",
       "4    The system must associate test examinations co...\n",
       "Name: Review_Text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review_Text'] = df['Review_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['Review_Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\b(?:https?://|www\\.)\\S+\\b', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    text = re.sub(r'\\b(cloud-based)\\b', 'cloud_based', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatize and remove stopwords\n",
    "    return ' '.join(words)  # Return cleaned text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['Cleaned_Text'] = df['Review_Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group text by category (Label)\n",
    "category_texts = df.groupby('Label')['Cleaned_Text'].apply(lambda x: ' '.join(x)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results dictionary\n",
    "seed_words = {}\n",
    "fixed_word_count = 30  # Ensure exactly 30 words per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract seed words using enhanced TF-IDF and optimized RAKE\n",
    "word_weights = Counter()\n",
    "category_word_scores = {}\n",
    "\n",
    "for category, text in category_texts.items():\n",
    "    # --- Enhanced TF-IDF Extraction ---\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=50,          # Increased from 10 to capture more relevant terms\n",
    "        ngram_range=(1, 2),       # Consider both single words and bigrams\n",
    "        stop_words=list(stop_words),\n",
    "        min_df=1,                 # Ignore terms appearing in <3 documents\n",
    "        max_df=1.0                # Remove terms in >70% of documents\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    tfidf_terms = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # --- Optimized RAKE Extraction ---\n",
    "    rake = Rake(\n",
    "        stopwords=stop_words,\n",
    "        min_length=2,             # Minimum phrase length\n",
    "        max_length=3,             # Maximum phrase length\n",
    "        ranking_metric=Metric.WORD_FREQUENCY,  # Focus on term importance\n",
    "        include_repeated_phrases=False  # Avoid duplicate phrases\n",
    "    )\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    rake_phrases = rake.get_ranked_phrases()[:8]  # Get more phrases for better coverage\n",
    "    \n",
    "    # Process RAKE phrases to handle multi-word terms\n",
    "    rake_terms = []\n",
    "    for phrase in rake_phrases:\n",
    "        # Split phrases and lemmatize components\n",
    "        processed_phrase = [lemmatizer.lemmatize(word) for word in phrase.split()]\n",
    "        rake_terms.extend(processed_phrase)\n",
    "    \n",
    "    # Combine and filter terms\n",
    "    combined_terms = list(set(tfidf_terms) | set(rake_terms))\n",
    "    filtered_terms = [\n",
    "        word for word in combined_terms \n",
    "        if word.isalpha() \n",
    "        and word not in stop_words\n",
    "        and len(word) > 2  # Filter short words\n",
    "    ]\n",
    "    \n",
    "    # Compute weighted frequencies\n",
    "    word_frequencies = Counter(filtered_terms)\n",
    "    word_weights.update(word_frequencies)\n",
    "    category_word_scores[category] = word_frequencies\n",
    "    \n",
    "    # Dynamic word count adjustment\n",
    "    category_word_count = max(min(len(filtered_terms), 15), 50)  # Flexible count\n",
    "    if len(filtered_terms) < category_word_count:\n",
    "        # Add contextually relevant filler words\n",
    "        freq_words = [word for word, _ in Counter(text.split()).most_common(30)]\n",
    "        additional_words = [w for w in freq_words \n",
    "                           if w not in filtered_terms \n",
    "                           and w not in stop_words][:category_word_count - len(filtered_terms)]\n",
    "        filtered_terms.extend(additional_words)\n",
    "    \n",
    "    seed_words[category] = filtered_terms[:category_word_count]\n",
    "\n",
    "# Enhanced cross-category deduplication\n",
    "final_seed_words = {category: [] for category in seed_words}\n",
    "word_category_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, words in seed_words.items():\n",
    "    for word in words:\n",
    "        if word in word_to_category:\n",
    "            prev_category = word_to_category[word]\n",
    "            \n",
    "            # Only proceed if word exists in previous category's list\n",
    "            if word in final_seed_words[prev_category]:\n",
    "                current_score = category_word_scores[category].get(word, 0)\n",
    "                prev_score = category_word_scores[prev_category].get(word, 0)\n",
    "                \n",
    "                if current_score > prev_score:\n",
    "                    # Safely remove from previous category\n",
    "                    final_seed_words[prev_category].remove(word)\n",
    "                    final_seed_words[category].append(word)\n",
    "                    word_to_category[word] = category\n",
    "        else:\n",
    "            final_seed_words[category].append(word)\n",
    "            word_to_category[word] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: BWR -> Seed Words: ['teacher', 'list', 'administrator', 'activity', 'defensive', 'available', 'shot', 'district', 'result', 'participant', 'meeting', 'process', 'material', 'staff', 'report', 'assessment', 'game', 'create', 'development', 'plan']\n",
      "Category: COM -> Seed Words: ['version', 'compatible', 'internet', 'client', 'current', 'phone', 'database', 'format', 'component', 'party', 'software', 'standard', 'multiple']\n",
      "Category: PE -> Seed Words: ['day', 'website', 'every', 'load', 'service', 'processing', 'maximum', 'returned', 'longer', 'lead', 'performance', 'take', 'let', 'less', 'connection', 'customer', 'application', 'movie', 'server', 'without', 'task']\n",
      "Category: U -> Seed Words: ['easy', 'language', 'page', 'dispute', 'successfully', 'cardmember', 'used', 'one', 'help', 'problem']\n",
      "Category: UIR -> Seed Words: ['shipping', 'profile', 'individual', 'table', 'order', 'include', 'lab', 'allows', 'different', 'case', 'detailed', 'option', 'displayed', 'screen', 'site']\n",
      "Category: UInR -> Seed Words: ['recycled', 'enter', 'rating', 'select', 'streaming', 'notify', 'part', 'preferred', 'enable']\n"
     ]
    }
   ],
   "source": [
    "# Display the final extracted seed words\n",
    "for category, words in final_seed_words.items():\n",
    "    print(f\"Category: {category} -> Seed Words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
